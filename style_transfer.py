# -*- coding: utf-8 -*-
"""style_transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yi8dNXFaw-_wDRQg7wyH3yOzgtoVWSyX
"""

from __future__ import print_function

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
from PIL import Image
from IPython.display import display
import matplotlib.pyplot as plt
from torchvision import datasets
import torchvision.transforms as transforms
import torchvision.models as models
import torchsummary
import numpy as np
from collections import namedtuple

import copy
import time
import glob
import re
import cv2
import torchvision

from tqdm.notebook import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount("/content/gdrive")

# field 변수 정의
style_img_loc = "/content/gdrive/My Drive/Colab Notebooks/data/The_Scream.jpg"
content_img_path = "/content/gdrive/My Drive/Colab Notebooks/data/IMG_3936.JPG"
test_img_path = "/content/gdrive/My Drive/Colab Notebooks/data/IMG_3907.jpg"

batch_size = 8
num_epochs = 64
learning_rate = 1e-3

content_weight = 1e5
style_weight = 1e10

ckpt_dir = "/content/gdrive/My Drive/Colab Notebooks/data/check_point"
img_path = "/content/gdrive/My Drive/Colab Notebooks/data/output/unet_scream_content_1e5_style_1e10"
test_image_dir = "/content/gdrive/My Drive/Colab Notebooks/data/output"

mckpt_model_path = os.path.join(ckpt_dir, "ckpt_epoch_63_batch_id_313.pth")

isTraining = False

if isTraining:
    os.mkdir(img_path)

class VGG16(nn.Module):
    def __init__(self, requires_grad=False):
        super(VGG16, self).__init__()

        vgg_pretrained_features = models.vgg16(pretrained=True).features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()

        for x in range(4):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])
        for x in range(4, 9):
            self.slice2.add_module(str(x), vgg_pretrained_features[x])
        for x in range(9, 16):
            self.slice3.add_module(str(x), vgg_pretrained_features[x])
        for x in range(16, 23):
            self.slice4.add_module(str(x), vgg_pretrained_features[x])
        
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
        h = self.slice1(X)
        h_relu1_2 = h
        h = self.slice2(h)
        h_relu2_2 = h
        h = self.slice3(h)
        h_relu3_3 = h
        h = self.slice4(h)
        h_relu4_3 = h

        vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])
        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)

        return out

class ConvReLuBlock(nn.Module):
    def __init__(self, in_chan, out_chan):
        super(ConvReLuBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_chan, out_chan, kernel_size=3, padding=1),
            nn.InstanceNorm2d(out_chan),
            nn.ReLU()
        )

    def forward(self,x):
        return self.conv(x)

class DownConvBlock(nn.Module):
    def __init__(self, in_chan, out_chan):
        super(DownConvBlock, self).__init__()
        
        self.conv = nn.Sequential(
            ConvReLuBlock(in_chan, out_chan),
            ConvReLuBlock(out_chan, out_chan)
        )

        self.down_conv = nn.Sequential(
            nn.Conv2d(out_chan, out_chan, kernel_size=3, padding=1),
            nn.MaxPool2d(2,2),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.down_conv(x)
        return x


class UpConvBlock(nn.Module):
    def __init__(self, in_chan, skip_chan, out_chan):
        super(UpConvBlock, self).__init__()

        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.conv1x1 = nn.Conv2d(in_chan + skip_chan, out_chan, kernel_size=1)
        
        self.conv = nn.Sequential(
            ConvReLuBlock(out_chan, out_chan),
            ConvReLuBlock(out_chan, out_chan),
        )

    def forward(self, x, skip):
        x = self.conv1x1(torch.cat([skip, self.upsample(x)], dim=1))
        x = self.conv(x)
        return x
	
class TransformerNet(nn.Module):
    def __init__(self, num_pool_layer, feature_chan, in_chan, out_chan, num_mid_conv):
        super(TransformerNet, self).__init__()

        self.in_conv = ConvReLuBlock(in_chan, feature_chan)

        chan = feature_chan
        stack = list()

        self.down_convs = nn.ModuleList()
        for _ in range(num_pool_layer):
            self.down_convs.append(DownConvBlock(chan, chan * 2))
            stack.append(chan)
            chan = chan * 2

        self.mid_convs = nn.ModuleList()
        for _ in range(num_mid_conv):
            self.mid_convs.append(ConvReLuBlock(chan, chan))

        self.up_convs = nn.ModuleList()
        for _ in range(num_pool_layer):
            self.up_convs.append(UpConvBlock(chan, stack.pop(), chan // 2))
            chan = chan // 2

        self.conv1x1 = nn.Conv2d(chan, out_chan, kernel_size=1, padding=0)
        
    def forward(self, x):
        x = self.in_conv(x)
        
        stack = list()
        for layer in self.down_convs:
            stack.append(x)
            x = layer(x)

        for layer in self.mid_convs:
            x = x + layer(x)

        for layer in self.up_convs:
            x = layer(x, stack.pop())

        return self.conv1x1(x)

def data_loader(img_path):
    img = Image.open(img_path)
    return img
    
def gram_matrix(y):
    (b, ch, h, w) = y.size()
    features = y.view(b, ch, w * h)
    features_t = features.transpose(1, 2)
    gram = features.bmm(features_t) / (ch * h * w)
    return gram

def normalize_batch(batch):
    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1)
    std = batch.new_tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1)
    return (batch - mean) / std

np.random.seed(10)
torch.manual_seed(10)

mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

transform = transforms.Compose([
    transforms.Resize(256),           
    transforms.CenterCrop(256),             
    transforms.ToTensor(),  
    transforms.Normalize(mean=mean, std=std),
])

style_transform = transforms.Compose([
    transforms.Resize(256),           
    transforms.CenterCrop(256), 
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])

content_transform = transforms.Compose([
    transforms.Resize(512), 
    transforms.CenterCrop(512),             
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])

train_dataset = datasets.ImageFolder("/content/gdrive/My Drive/Colab Notebooks/data/COCO", transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, num_workers=1)

transformer = TransformerNet(4, 3, 3, 3, 3).to(device)
vgg = VGG16(requires_grad=False).to(device)

optimizer = torch.optim.Adam(transformer.parameters(), learning_rate)
mse_loss = nn.MSELoss()

style = data_loader(style_img_loc)
style = style_transform(style)
# Repeats this tensor along the specified dimensions.
style = style.repeat(batch_size, 1, 1, 1).to(device)

content_image = data_loader(img_path=content_img_path)
content_image = content_transform(content_image)
content_image = content_image.unsqueeze(0).to(device)
content_image = (content_image - torch.min(content_image)) / (torch.max(content_image) - torch.min(content_image))

features_style = vgg(style)
gram_style = [gram_matrix(y) for y in features_style]

if isTraining:

    transfer_learning_epoch = 0

    for epoch in range(transfer_learning_epoch, num_epochs):
        
        transformer.train()
        agg_content_loss = 0.
        agg_style_loss = 0.

        for batch_id, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):
            
            n_batch = len(x)
            optimizer.zero_grad()

            x = x.to(device)
            y = transformer(x)
            
            # feature를 뽑아냄
            features_y = vgg(y)
            features_x = vgg(x)

            # error point
            content_loss = content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)

            style_loss = 0.
            for ft_y, gm_s in zip(features_y, gram_style):
                gm_y = gram_matrix(ft_y)
                style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])
            style_loss *= style_weight

            total_loss = content_loss + style_loss
            total_loss.backward()
            optimizer.step()

            # get a Python number from a tensor containing a single value
            agg_content_loss += content_loss.item()
            agg_style_loss += style_loss.item()

        # ?
        transformer.eval()
        ckpt_model_filename = "ckpt_epoch_" + str(epoch) + ".pth"
        mckpt_model_path = os.path.join(ckpt_dir, mckpt_model_path)
        
        torch.save({
        'epoch': epoch,
        'model_state_dict': transformer.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss
        }, mckpt_model_path)

        print("content loss: {:.6f}\tstyle loss: {:.6f}\ttotal loss: {:.6f}".format(
                                    agg_content_loss / (batch_id + 1),
                                    agg_style_loss / (batch_id + 1),
                                    (agg_content_loss + agg_style_loss) / (batch_id + 1)))

        output = transformer(content_image).to(device)
        torchvision.utils.save_image(output, img_path + '/output' + str(epoch) + '.png')

        transformer.to(device).train()

if not isTraining:
    
    # backword
    with torch.no_grad():
            style_model = TransformerNet(4, 3, 3, 3, 3).to(device)

            ckpt_model_path = os.path.join(ckpt_dir, mckpt_model_path)
            checkpoint = torch.load(ckpt_model_path, map_location=device)

            # remove saved deprecated running_* keys in InstanceNorm from the checkpoint
            for k in list(checkpoint.keys()):
                #  Finding Pattern in Text
                if re.search(r'in\d+\.running_(mean|var)$', k):
                    del checkpoint[k]

            style_model.load_state_dict(checkpoint['model_state_dict'])
            style_model.to(device)

            test_image = data_loader(test_img_path)
            test_image = content_transform(test_image)
            test_image = test_image.unsqueeze(0).to(device)
            test_image = (test_image - torch.min(test_image)) / (torch.max(test_image) - torch.min(test_image))

            output = style_model(test_image).cpu()

            torchvision.utils.save_image(output, test_image_dir + '/output_test_IMG_3907' + '.png')